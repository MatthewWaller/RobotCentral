# GR00T Performance Optimizations

This document explains the performance optimizations made to reduce inference time between robot actions.

## Problem

The original GR00T evaluation script (`eval_lerobot.py`) was experiencing slow inference times due to:

1. **ZeroMQ Client-Server Architecture**: Each action request involves network communication overhead
2. **Serialization/Deserialization**: PyTorch tensors are serialized/deserialized for each request
3. **Cold Start Penalty**: Model isn't warmed up, causing slower first inference
4. **Suboptimal Parameters**: Using 4 denoising steps and full action horizon

## Solution: Direct Inference (`so100_inference_optimized.py`)

### Key Optimizations

1. **üöÄ Eliminate Server-Client Overhead**
   - Load GR00T model directly in memory
   - No ZeroMQ communication
   - No serialization/deserialization

2. **‚ö° Reduce Denoising Steps**
   - Default: 4 steps ‚Üí Optimized: 2 steps
   - ~50% reduction in diffusion model computation
   - Configurable parameter

3. **üî• Model Warm-up**
   - Pre-warm model with dummy inference
   - Eliminates cold start penalty
   - Subsequent inferences are faster

4. **üìä Performance Monitoring**
   - Real-time inference timing
   - Loop timing for full action cycle
   - Easy to identify bottlenecks

5. **üîß Auto-Resolution Handling**
   - Automatically detects camera resolution mismatches
   - Resizes camera input to match model expectations
   - Eliminates "invalid resolution" errors

### Performance Comparison

| Method | Inference Time | Communication | Memory |
|--------|---------------|---------------|---------|
| **Original (Server-Client)** | ~0.5-1.0s | ZeroMQ overhead | Server + Client |
| **Optimized (Direct)** | ~0.1-0.3s | None | Single process |

**Expected improvement: 2-5x faster inference**

## Usage

### Original Method (Slow)
```bash
# Terminal 1: Start server
python isaac-gr00t/scripts/inference_service.py --server \
    --model_path ./downloaded_models/gr00t/checkpoints/gr00t-n1.5-so100-finetuned \
    --embodiment-tag new_embodiment \
    --data-config so100 \
    --denoising-steps 4

# Terminal 2: Run client
python isaac-gr00t/getting_started/examples/eval_lerobot.py \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodem58FA0921581 \
    --robot.id=black \
    --robot.cameras="{ front: {type: opencv, index_or_path: 1, width: 1280, height: 720, fps: 30}}" \
    --policy_host=localhost \
    --policy_port=5555 \
    --lang_instruction="Place the block in the bowl" \
    --action_horizon=8
```

### Optimized Method (Fast)
```bash
# Single terminal - Direct inference
python so100_inference_optimized.py \
    --robot.type=so100_follower \
    --robot.port=/dev/tty.usbmodem58FA0921581 \
    --robot.id=black \
    --robot.cameras="{ front: {type: opencv, index_or_path: 1, width: 1280, height: 720, fps: 30}}" \
    --model_path=./downloaded_models/gr00t/checkpoints/gr00t-n1.5-so100-finetuned \
    --embodiment_tag=new_embodiment \
    --data_config=so100 \
    --denoising_steps=2 \
    --lang_instruction="Place the block in the bowl" \
    --action_horizon=8
```

## Tuning Parameters

### Denoising Steps
- **Default**: 4 steps
- **Recommended**: 2 steps (faster, slight quality trade-off)
- **Minimum**: 1 step (fastest, potential quality loss)

```python
--denoising_steps=2  # Balance of speed and quality
```

### Action Horizon
- **Default**: 8 actions executed per inference
- **Alternative**: 4-6 actions for faster loops
- **Trade-off**: More frequent inference vs. smoother actions

```python
--action_horizon=6  # Faster action loops
```

### Device Selection
- **GPU**: `--device=cuda` (faster inference)
- **CPU**: `--device=cpu` (no GPU requirement)

## Additional Optimizations

### For Even Faster Performance

1. **Reduce Image Resolution**
   - Use smaller camera resolution (e.g., 640x480 instead of 1280x720)
   - Faster image processing and less memory

2. **Optimize Camera FPS**
   - Reduce camera FPS if not needed for task
   - Less image processing overhead

3. **Use TensorRT** (Advanced)
   - Convert model to TensorRT for faster GPU inference
   - Requires additional setup but can provide 2-3x speedup

4. **Quantization** (Advanced)
   - Use INT8 quantization for faster inference
   - Trade-off between speed and model accuracy

## Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   - Reduce denoising steps
   - Use smaller batch sizes
   - Switch to CPU if necessary

2. **Slow First Inference**
   - Model warm-up should handle this
   - Check if CUDA is properly initialized

3. **Model Loading Errors**
   - Verify model path exists
   - Check embodiment tag matches your model

4. **Resolution Mismatch (Fixed in Optimized Version)**
   - ‚ùå Old error: "Video has invalid resolution (640, 480), expected (1280, 720)"
   - ‚úÖ Optimized script automatically handles this
   - Resizes camera input to match model expectations

### Performance Monitoring

The optimized script provides real-time performance metrics:

```
‚ö° Direct inference time: 0.234s
üîÑ Loop 1: 0.398s total
üîÑ Loop 2: 0.245s total
```

- **Inference time**: Pure model forward pass
- **Loop time**: Complete observation ‚Üí action ‚Üí execution cycle

## Future Improvements

1. **Batch Processing**: Process multiple observations in parallel
2. **Model Caching**: Cache intermediate computations
3. **Asynchronous Processing**: Overlap model inference with robot actions
4. **Custom CUDA Kernels**: Optimize specific operations

---

**Bottom Line**: The optimized script should provide 2-5x faster inference times by eliminating server-client overhead and reducing denoising steps while maintaining task performance quality. 