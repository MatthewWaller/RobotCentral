{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "E0GW7qFSY4L7"
      },
      "source": [
        "# ü§ñ Fine-tuning GR00T N1.5 with Single Camera Setup\n",
        "\n",
        "This notebook demonstrates how to fine-tune **NVIDIA Isaac GR00T N1.5** using a dataset from Hugging Face with a single camera setup. GR00T N1.5 is the world's first open foundation model for generalized humanoid robot reasoning and skills.\n",
        "\n",
        "## üìã Overview\n",
        "\n",
        "In this tutorial, we will:\n",
        "1. üîß Set up the environment and install dependencies\n",
        "2. üì• Download a dataset from Hugging Face\n",
        "3. ‚öôÔ∏è Configure the dataset for single camera setup\n",
        "4. üéØ Fine-tune GR00T N1.5 on the dataset\n",
        "5. üìä Evaluate the fine-tuned model\n",
        "\n",
        "## ‚úÖ Prerequisites\n",
        "\n",
        "- Google Colab with GPU runtime (T4 or V100 recommended)\n",
        "- Basic Python knowledge\n",
        "- Understanding of machine learning concepts\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "w1ruezYeY4L8"
      },
      "source": [
        "## Step 1: üîß Environment Setup\n",
        "\n",
        "First, let's set up our environment by installing the necessary dependencies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13oEezAnY4L8"
      },
      "outputs": [],
      "source": [
        "# Check if we're running on GPU\n",
        "import torch\n",
        "print(f\"üîç CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Warning: No GPU detected. Training will be very slow!\")\n",
        "    print(\"Please change runtime type to GPU in Runtime > Change runtime type\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh5MTcRbY4L9"
      },
      "outputs": [],
      "source": [
        "# Clone the Isaac-GR00T repository\n",
        "print(\"üìÇ Cloning Isaac-GR00T repository...\")\n",
        "!git clone https://github.com/NVIDIA/Isaac-GR00T\n",
        "%cd Isaac-GR00T\n",
        "print(\"‚úÖ Repository cloned successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZBQNSFXY4L9"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "%pip install --upgrade setuptools\n",
        "%pip install -e .[base]\n",
        "%pip install --no-build-isolation flash-attn==2.7.1.post4\n",
        "%pip install huggingface_hub\n",
        "%pip install matplotlib\n",
        "%pip install tyro\n",
        "%pip install wandb\n",
        "print(\"‚úÖ Dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "894h4H-MY4L9"
      },
      "source": [
        "## Step 2: üîê Setup Authentication (Optional)\n",
        "\n",
        "Set up authentication for experiment tracking and model sharing. These steps are optional but recommended.\n",
        "\n",
        "### Weights & Biases Setup:\n",
        "1. Go to [wandb.ai](https://wandb.ai) and create a free account\n",
        "2. Get your API key from [wandb.ai/authorize](https://wandb.ai/authorize)\n",
        "3. In Colab, go to the key icon (üîë) on the left sidebar ‚Üí \"Secrets\"\n",
        "4. Add a new secret with name `WANDB_API_KEY` and paste your API key as the value\n",
        "5. Enable \"Notebook access\" for this secret\n",
        "\n",
        "### Hugging Face Hub Setup:\n",
        "1. Go to [huggingface.co](https://huggingface.co) and create a free account\n",
        "2. Get your access token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
        "3. Create a new token with \"Write\" permissions\n",
        "4. In Colab Secrets, add a new secret with name `HUGGINGFACE_TOKEN` and paste your token\n",
        "5. Enable \"Notebook access\" for this secret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwFHvi-ZY4L9"
      },
      "outputs": [],
      "source": [
        "# Setup Weights & Biases (optional)\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "    wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "\n",
        "    if wandb_api_key:\n",
        "        import wandb\n",
        "        wandb.login(key=wandb_api_key)\n",
        "        print(\"‚úÖ Successfully logged into Weights & Biases!\")\n",
        "        use_wandb = True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è WANDB_API_KEY not found in secrets. Skipping wandb setup.\")\n",
        "        print(\"üí° You can add it later in Colab Secrets for experiment tracking.\")\n",
        "        use_wandb = False\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not setup wandb: {e}\")\n",
        "    print(\"üí° Training will continue without wandb tracking.\")\n",
        "    use_wandb = False\n",
        "\n",
        "# Setup Hugging Face Hub (optional)\n",
        "try:\n",
        "    from huggingface_hub import login\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"‚úÖ Successfully logged into Hugging Face Hub!\")\n",
        "        use_hf_hub = True\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è HUGGINGFACE_TOKEN not found in secrets. Skipping HF Hub setup.\")\n",
        "        print(\"üí° You can add it later in Colab Secrets for model sharing.\")\n",
        "        use_hf_hub = False\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not setup Hugging Face Hub: {e}\")\n",
        "    print(\"üí° Model will not be automatically pushed to Hugging Face.\")\n",
        "    use_hf_hub = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hprHWdI4Y4L9"
      },
      "outputs": [],
      "source": [
        "## Step 3: üì• Dataset Preparation\n",
        "\n",
        "# Create directory for datasets\n",
        "!mkdir -p demo_data\n",
        "print(\"üìÅ Created demo_data directory\")\n",
        "\n",
        "# Download a sample dataset from Hugging Face\n",
        "# You can replace this with your own dataset\n",
        "print(\"‚¨áÔ∏è Downloading dataset from Hugging Face...\")\n",
        "!huggingface-cli download \\\n",
        "    --repo-type dataset HelloCephalopod/block_pickup_17 \\\n",
        "    --local-dir ./demo_data/block_pickup_17\n",
        "print(\"‚úÖ Dataset downloaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u9y9LrrY4L-"
      },
      "outputs": [],
      "source": [
        "# Copy the single camera modality configuration\n",
        "print(\"‚öôÔ∏è Setting up single camera modality configuration...\")\n",
        "!cp getting_started/examples/so100__modality.json ./demo_data/so101-table-cleanup/meta/modality.json\n",
        "print(\"‚úÖ Modality configuration set up!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6DN3DPNY4L-"
      },
      "outputs": [],
      "source": [
        "# Let's examine the dataset structure\n",
        "print(\"üîç Dataset structure:\")\n",
        "!ls -la ./demo_data/block_pickup_17\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ot2yRlBY4L-"
      },
      "outputs": [],
      "source": [
        "# Check the modality configuration\n",
        "import json\n",
        "with open('./demo_data/block_pickup_17/meta/modality.json', 'r') as f:\n",
        "    modality_config = json.load(f)\n",
        "print(\"‚öôÔ∏è Modality Configuration:\")\n",
        "print(json.dumps(modality_config, indent=2))\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "GO0JfWn5Y4L-"
      },
      "source": [
        "## Step 4: üëÅÔ∏è Dataset Loading and Visualization\n",
        "\n",
        "Let's load the dataset and visualize some samples to understand the data structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlclGHLtY4L-"
      },
      "outputs": [],
      "source": [
        "# Add the gr00t module to Python path\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "\n",
        "from gr00t.data.dataset import LeRobotSingleDataset\n",
        "from gr00t.data.embodiment_tags import EmbodimentTag\n",
        "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "print(\"‚úÖ Modules imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQBEUu4HY4L-"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "print(\"üìä Loading dataset...\")\n",
        "dataset_path = \"./demo_data/block_pickup_17\"\n",
        "embodiment_tag = EmbodimentTag(\"new_embodiment\")\n",
        "\n",
        "# Get data configuration for single camera setup\n",
        "data_config_cls = DATA_CONFIG_MAP[\"so100\"]\n",
        "modality_configs = data_config_cls.modality_config()\n",
        "transforms = data_config_cls.transform()\n",
        "\n",
        "# Create dataset\n",
        "dataset = LeRobotSingleDataset(\n",
        "    dataset_path=dataset_path,\n",
        "    modality_configs=modality_configs,\n",
        "    transforms=transforms,\n",
        "    embodiment_tag=embodiment_tag,\n",
        "    video_backend=\"torchvision_av\",\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"üìà Number of episodes: {len(dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbWGiRAWY4L-"
      },
      "outputs": [],
      "source": [
        "# Visualize a sample from the dataset\n",
        "print(\"üé¨ Visualizing dataset sample...\")\n",
        "sample = dataset[0]\n",
        "print(\"üîë Sample keys:\", list(sample.keys()))\n",
        "\n",
        "# Display video frames\n",
        "if 'video.webcam' in sample:\n",
        "    video_frames = sample['video.webcam']\n",
        "    print(f\"üìπ Video shape: {video_frames.shape}\")\n",
        "\n",
        "    # Show first few frames\n",
        "    fig, axes = plt.subplots(1, min(5, video_frames.shape[0]), figsize=(15, 3))\n",
        "    if video_frames.shape[0] == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for i, ax in enumerate(axes[:min(5, video_frames.shape[0])]):\n",
        "        frame = video_frames[i].numpy()\n",
        "        # Convert from CxHxW to HxWxC\n",
        "        frame = np.transpose(frame, (1, 2, 0))\n",
        "        # Normalize to 0-1 range\n",
        "        frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
        "        ax.imshow(frame)\n",
        "        ax.set_title(f'Frame {i}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display state and action information\n",
        "if 'state.single_arm' in sample:\n",
        "    print(f\"ü¶æ State shape: {sample['state.single_arm'].shape}\")\n",
        "    print(f\"üìä State values: {sample['state.single_arm'][0]}\")\n",
        "\n",
        "if 'action.single_arm' in sample:\n",
        "    print(f\"üéØ Action shape: {sample['action.single_arm'].shape}\")\n",
        "    print(f\"üìä Action values: {sample['action.single_arm'][0]}\")\n",
        "\n",
        "if 'annotation.human.task_description' in sample:\n",
        "    print(f\"üìù Task description: {sample['annotation.human.task_description']}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "duCTsqiCY4L-"
      },
      "source": [
        "## Step 5: ‚öôÔ∏è Fine-tuning Configuration\n",
        "\n",
        "Now let's set up the fine-tuning configuration. We'll use a configuration suitable for single camera setup and Colab's resource constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWhsLskpY4L-"
      },
      "outputs": [],
      "source": [
        "# Create output directory for checkpoints\n",
        "!mkdir -p ./checkpoints\n",
        "print(\"üìÅ Created checkpoints directory\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elRuDB2MY4L-"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning configuration\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Set environment variables for better performance\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "\n",
        "# Configure wandb reporting based on setup\n",
        "report_to = 'wandb' if use_wandb else 'none'\n",
        "\n",
        "# Fine-tuning command optimized for Colab\n",
        "cmd = [\n",
        "    'python', 'scripts/gr00t_finetune.py',\n",
        "    '--dataset-path', './demo_data/block_pickup_17',\n",
        "    '--num-gpus', '1',\n",
        "    '--output-dir', './checkpoints',\n",
        "    '--max-steps', '1000',  # Reduced for demo purposes\n",
        "    '--data-config', 'so100',  # Single camera configuration\n",
        "    '--video-backend', 'torchvision_av',\n",
        "    '--batch-size', '8',  # Reduced batch size for Colab\n",
        "    '--save-steps', '200',\n",
        "    '--learning-rate', '1e-4',\n",
        "    '--no-tune_diffusion_model',  # Disable diffusion model tuning to save memory\n",
        "    '--report-to', report_to  # Use wandb if available, otherwise none\n",
        "]\n",
        "\n",
        "print(\"üéØ Fine-tuning command configured:\")\n",
        "print(' '.join(cmd))\n",
        "print(\"\\n‚öôÔ∏è Configuration optimized for:\")\n",
        "print(\"   ‚Ä¢ Single camera setup (so100 config)\")\n",
        "print(\"   ‚Ä¢ Colab GPU memory constraints\")\n",
        "print(\"   ‚Ä¢ Reduced training steps for demo\")\n",
        "print(f\"   ‚Ä¢ Experiment tracking: {'Weights & Biases' if use_wandb else 'Disabled'}\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "AAnCdCx5Y4L_"
      },
      "source": [
        "## Step 6: üöÄ Start Fine-tuning\n",
        "\n",
        "Now let's start the fine-tuning process. This will take some time depending on your hardware.\n",
        "\n",
        "If you set up Weights & Biases, you'll be able to monitor training progress in real-time at [wandb.ai](https://wandb.ai).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmYCHeWwY4L_"
      },
      "outputs": [],
      "source": [
        "# Start fine-tuning\n",
        "print(\"üöÄ Starting fine-tuning...\")\n",
        "print(\"‚è≥ This may take a while. You can monitor progress in the output below.\")\n",
        "print(\"‚òï Time for a coffee break!\\n\")\n",
        "\n",
        "try:\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
        "    print(\"üéâ Fine-tuning completed successfully!\")\n",
        "    print(\"üìä Output:\", result.stdout)\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"‚ùå Fine-tuning failed with error: {e}\")\n",
        "    print(f\"üîç Error output: {e.stderr}\")\n",
        "    print(\"\\nüîß Trying with reduced memory settings...\")\n",
        "\n",
        "    # Try with even more reduced settings\n",
        "    cmd_reduced = cmd + ['--batch-size', '4', '--tune_projector', 'False']\n",
        "    try:\n",
        "        result = subprocess.run(cmd_reduced, capture_output=True, text=True, check=True)\n",
        "        print(\"üéâ Fine-tuning completed successfully with reduced settings!\")\n",
        "        print(\"üìä Output:\", result.stdout)\n",
        "    except subprocess.CalledProcessError as e2:\n",
        "        print(f\"‚ùå Fine-tuning still failed: {e2}\")\n",
        "        print(f\"üîç Error output: {e2.stderr}\")\n",
        "        print(\"\\nüí° Try reducing batch size further or using a different GPU runtime.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "K_5gd-eAY4L_"
      },
      "source": [
        "## Step 7: üìä Check Training Progress\n",
        "\n",
        "Let's check if the training completed and examine the checkpoints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rN24CIq5Y4L_"
      },
      "outputs": [],
      "source": [
        "# Check if checkpoints were created\n",
        "print(\"üìÅ Checking for checkpoints...\")\n",
        "!ls -la ./checkpoints/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKJ5YLTPY4L_"
      },
      "outputs": [],
      "source": [
        "# Find the latest checkpoint\n",
        "import glob\n",
        "checkpoint_dirs = glob.glob('./checkpoints/checkpoint-*')\n",
        "if checkpoint_dirs:\n",
        "    latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
        "    print(f\"‚úÖ Latest checkpoint found: {latest_checkpoint}\")\n",
        "    !ls -la {latest_checkpoint}\n",
        "else:\n",
        "    print(\"‚ùå No checkpoints found. Training may have failed or not completed yet.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "ztaexF7rY4L_"
      },
      "source": [
        "## Step 8: üìà Model Evaluation\n",
        "\n",
        "If training completed successfully, let's evaluate the fine-tuned model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKBp4ux-Y4L_"
      },
      "outputs": [],
      "source": [
        "# Evaluate the fine-tuned model (if checkpoints exist)\n",
        "if checkpoint_dirs:\n",
        "    latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
        "\n",
        "    eval_cmd = [\n",
        "        'python', 'scripts/eval_policy.py',\n",
        "        '--plot',\n",
        "        '--embodiment_tag', 'new_embodiment',\n",
        "        '--model_path', latest_checkpoint,\n",
        "        '--data_config', 'so100',\n",
        "        '--dataset_path', './demo_data/block_pickup_17',\n",
        "        '--video_backend', 'torchvision_av',\n",
        "        '--modality_keys', 'single_arm', 'gripper'\n",
        "    ]\n",
        "\n",
        "    print(\"üìä Evaluating fine-tuned model...\")\n",
        "    print(' '.join(eval_cmd))\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(eval_cmd, capture_output=True, text=True, check=True)\n",
        "        print(\"üéâ Evaluation completed successfully!\")\n",
        "        print(\"üìä Output:\", result.stdout)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Evaluation failed: {e}\")\n",
        "        print(f\"üîç Error output: {e.stderr}\")\n",
        "else:\n",
        "    print(\"‚ùå No checkpoints available for evaluation.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "bM4COTPpY4L_"
      },
      "source": [
        "## Step 9: üíæ Save and Download Model\n",
        "\n",
        "If you want to save your fine-tuned model, you can download it from Colab.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B89eniE3Y4L_"
      },
      "outputs": [],
      "source": [
        "# Create a zip file of the checkpoints for download\n",
        "if checkpoint_dirs:\n",
        "    print(\"üì¶ Creating zip file for download...\")\n",
        "    !zip -r gr00t_finetuned_model.zip ./checkpoints/\n",
        "    print(\"‚úÖ Model checkpoints zipped as 'gr00t_finetuned_model.zip'\")\n",
        "    print(\"üì• You can download this file from the Colab file browser.\")\n",
        "\n",
        "    # Show file size\n",
        "    !ls -lh gr00t_finetuned_model.zip\n",
        "else:\n",
        "    print(\"‚ùå No checkpoints to save.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "lKCfqeTsY4L_"
      },
      "source": [
        "## Step 10: üöÄ Push Model to Hugging Face Hub\n",
        "\n",
        "If you set up Hugging Face authentication, we can automatically push your fine-tuned model to the Hub for sharing and future use.\n",
        "\n",
        "## üìã Summary\n",
        "\n",
        "In this notebook, we have:\n",
        "\n",
        "1. ‚úÖ Set up the environment with Isaac-GR00T\n",
        "2. ‚úÖ Configured authentication for Weights & Biases and Hugging Face\n",
        "3. ‚úÖ Downloaded a dataset from Hugging Face\n",
        "4. ‚úÖ Configured the dataset for single camera setup\n",
        "5. ‚úÖ Loaded and visualized the dataset\n",
        "6. ‚úÖ Fine-tuned GR00T N1.5 on the dataset with monitoring\n",
        "7. ‚úÖ Evaluated the fine-tuned model\n",
        "8. ‚úÖ Saved the model locally\n",
        "9. ‚úÖ Automatically pushed the model to Hugging Face Hub\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "To extend this work, you can:\n",
        "\n",
        "- **üîÑ Use your own dataset**: Replace the Hugging Face dataset with your own robot data\n",
        "- **‚öôÔ∏è Experiment with hyperparameters**: Try different learning rates, batch sizes, and training steps\n",
        "- **üìä Advanced monitoring**: Set up custom wandb metrics and alerts\n",
        "- **ü§ñ Deploy to real hardware**: Use the fine-tuned model with actual robot hardware\n",
        "- **üìπ Multi-camera setup**: Modify the modality configuration for multiple cameras\n",
        "- **ü¶æ Different robot embodiments**: Adapt the configuration for different robot types\n",
        "- **üî¨ Hyperparameter sweeps**: Use wandb sweeps to automatically find optimal hyperparameters\n",
        "- **üåê Share your models**: Upload your fine-tuned models to Hugging Face for the community\n",
        "- **üîÑ Model versioning**: Use Hugging Face's model versioning for iterative improvements\n",
        "\n",
        "## üìö Resources\n",
        "\n",
        "- [Isaac-GR00T GitHub Repository](https://github.com/NVIDIA/Isaac-GR00T)\n",
        "- [GR00T N1.5 Technical Blog](https://research.nvidia.com/labs/gear/gr00t-n1_5/)\n",
        "- [LeRobot Framework](https://github.com/huggingface/lerobot)\n",
        "- [Hugging Face Datasets](https://huggingface.co/datasets)\n",
        "- [NVIDIA GR00T Models on Hugging Face](https://huggingface.co/nvidia)\n",
        "- [Weights & Biases Documentation](https://docs.wandb.ai/)\n",
        "- [Weights & Biases for ML Experiments](https://wandb.ai/site/experiment-tracking)\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You've successfully fine-tuned GR00T N1.5 with a single camera setup and experiment tracking! ü§ñ‚ú®\n",
        "\n",
        "**Happy fine-tuning and robot building!** üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLJgI15YY4L_"
      },
      "outputs": [],
      "source": [
        "# Push fine-tuned model to Hugging Face Hub\n",
        "if use_hf_hub and checkpoint_dirs:\n",
        "    latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
        "\n",
        "    # Get user input for model repository name\n",
        "    import getpass\n",
        "    print(\"üè∑Ô∏è Model Repository Setup\")\n",
        "    print(\"Enter details for your Hugging Face model repository:\")\n",
        "\n",
        "    # You can customize these or make them interactive\n",
        "    default_username = \"your-username\"  # Users should replace this\n",
        "    default_model_name = \"gr00t-n1.5-so100-finetuned\"\n",
        "\n",
        "    print(f\"üìù Suggested repository name: {default_username}/{default_model_name}\")\n",
        "    print(\"üí° You can change this in the code above if needed\")\n",
        "\n",
        "    repo_name = f\"{default_username}/{default_model_name}\"\n",
        "\n",
        "    try:\n",
        "        from huggingface_hub import HfApi, create_repo\n",
        "\n",
        "        # Create repository if it doesn't exist\n",
        "        print(f\"üìÅ Creating repository: {repo_name}\")\n",
        "        create_repo(repo_id=repo_name, exist_ok=True, private=False)\n",
        "\n",
        "        # Upload the model files\n",
        "        api = HfApi()\n",
        "        print(f\"‚¨ÜÔ∏è Uploading model files from {latest_checkpoint}...\")\n",
        "\n",
        "        # Upload all files in the checkpoint directory\n",
        "        api.upload_folder(\n",
        "            folder_path=latest_checkpoint,\n",
        "            repo_id=repo_name,\n",
        "            repo_type=\"model\",\n",
        "            commit_message=f\"Fine-tuned GR00T N1.5 model on SO-101 dataset\"\n",
        "        )\n",
        "\n",
        "        # Create a model card\n",
        "        model_card_content = f\"\"\"---\n",
        "license: apache-2.0\n",
        "base_model: nvidia/GR00T-N1.5-3B\n",
        "tags:\n",
        "- robotics\n",
        "- gr00t\n",
        "- fine-tuned\n",
        "- so-101\n",
        "- single-camera\n",
        "library_name: transformers\n",
        "---\n",
        "\n",
        "# GR00T N1.5 Fine-tuned on SO-101 Dataset\n",
        "\n",
        "This model is a fine-tuned version of [nvidia/GR00T-N1.5-3B](https://huggingface.co/nvidia/GR00T-N1.5-3B) on a SO-101 robot arm dataset with single camera setup.\n",
        "\n",
        "## Model Details\n",
        "\n",
        "- **Base Model**: nvidia/GR00T-N1.5-3B\n",
        "- **Fine-tuned on**: SO-101 table cleanup dataset\n",
        "- **Camera Setup**: Single camera (webcam)\n",
        "- **Embodiment**: SO-101 robot arm\n",
        "- **Training Steps**: 1000 (demo configuration)\n",
        "\n",
        "## Usage\n",
        "\n",
        "```python\n",
        "from gr00t.model.gr00t_n1 import GR00T_N1_5\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model = GR00T_N1_5.from_pretrained(\"{repo_name}\")\n",
        "\n",
        "# Use for inference on SO-101 robot\n",
        "# ... your inference code here\n",
        "```\n",
        "\n",
        "## Training Details\n",
        "\n",
        "- **Training Data**: SO-100 pick and place block task\n",
        "- **Training Steps**: 1000\n",
        "- **Batch Size**: 8\n",
        "- **Learning Rate**: 1e-4\n",
        "- **Hardware**: Google Colab GPU\n",
        "\n",
        "## Intended Use\n",
        "\n",
        "This model is intended for research and educational purposes with SO-101 robot arms performing table cleanup tasks.\n",
        "\n",
        "## Limitations\n",
        "\n",
        "- Trained on a small dataset (demo purposes)\n",
        "- Single camera setup only\n",
        "- Specific to SO-100 embodiment\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this model, please cite the original GR00T paper and acknowledge the fine-tuning work.\n",
        "\"\"\"\n",
        "\n",
        "        # Upload model card\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=model_card_content.encode(),\n",
        "            path_in_repo=\"README.md\",\n",
        "            repo_id=repo_name,\n",
        "            repo_type=\"model\",\n",
        "            commit_message=\"Add model card\"\n",
        "        )\n",
        "\n",
        "        print(f\"üéâ Successfully uploaded model to: https://huggingface.co/{repo_name}\")\n",
        "        print(f\"üîó Model URL: https://huggingface.co/{repo_name}\")\n",
        "        print(\"‚úÖ Model card created with training details\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to upload to Hugging Face: {e}\")\n",
        "        print(\"üí° You can manually upload the model files later\")\n",
        "\n",
        "elif not use_hf_hub:\n",
        "    print(\"‚ö†Ô∏è Hugging Face Hub not configured. Skipping model upload.\")\n",
        "    print(\"üí° Set up HUGGINGFACE_TOKEN in Colab Secrets to enable automatic uploads\")\n",
        "elif not checkpoint_dirs:\n",
        "    print(\"‚ùå No model checkpoints found to upload.\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è Model upload skipped.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "HoFOpttUY4L_"
      },
      "source": [
        "## üìä Monitoring Training with Weights & Biases\n",
        "\n",
        "If you set up Weights & Biases, you can monitor your training progress in real-time:\n",
        "\n",
        "1. **Visit your wandb dashboard**: Go to [wandb.ai](https://wandb.ai) and navigate to your project\n",
        "2. **Monitor key metrics**:\n",
        "   - Training loss\n",
        "   - Learning rate schedule\n",
        "   - GPU utilization\n",
        "   - Training speed (steps/sec)\n",
        "3. **Compare experiments**: Try different hyperparameters and compare results\n",
        "4. **Share results**: Generate reports and share with your team\n",
        "\n",
        "### Key Metrics to Watch:\n",
        "- **Loss should decrease**: Training loss should trend downward over time\n",
        "- **No overfitting**: Validation loss (if available) shouldn't increase while training loss decreases\n",
        "- **Stable training**: Loss shouldn't oscillate wildly or explode\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}