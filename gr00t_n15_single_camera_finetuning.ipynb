{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ğŸ¤– Fine-tuning GR00T N1.5 with Single Camera Setup\n",
    "\n",
    "This notebook demonstrates how to fine-tune **NVIDIA Isaac GR00T N1.5** using a dataset from Hugging Face with a single camera setup. GR00T N1.5 is the world's first open foundation model for generalized humanoid robot reasoning and skills.\n",
    "\n",
    "## ğŸ“‹ Overview\n",
    "\n",
    "In this tutorial, we will:\n",
    "1. ğŸ”§ Set up the environment and install dependencies\n",
    "2. ğŸ“¥ Download a dataset from Hugging Face\n",
    "3. âš™ï¸ Configure the dataset for single camera setup\n",
    "4. ğŸ¯ Fine-tune GR00T N1.5 on the dataset\n",
    "5. ğŸ“Š Evaluate the fine-tuned model\n",
    "\n",
    "## âœ… Prerequisites\n",
    "\n",
    "- Google Colab with GPU runtime (T4 or V100 recommended)\n",
    "- Basic Python knowledge\n",
    "- Understanding of machine learning concepts\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: ğŸ”§ Environment Setup\n",
    "\n",
    "First, let's set up our environment by installing the necessary dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're running on GPU\n",
    "import torch\n",
    "print(f\"ğŸ” CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ Warning: No GPU detected. Training will be very slow!\")\n",
    "    print(\"Please change runtime type to GPU in Runtime > Change runtime type\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the Isaac-GR00T repository\n",
    "print(\"ğŸ“‚ Cloning Isaac-GR00T repository...\")\n",
    "!git clone https://github.com/NVIDIA/Isaac-GR00T\n",
    "%cd Isaac-GR00T\n",
    "print(\"âœ… Repository cloned successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"ğŸ“¦ Installing dependencies...\")\n",
    "%pip install --upgrade setuptools\n",
    "%pip install -e .[base]\n",
    "%pip install --no-build-isolation flash-attn==2.7.1.post4\n",
    "%pip install huggingface_hub\n",
    "%pip install matplotlib\n",
    "%pip install tyro\n",
    "%pip install wandb\n",
    "print(\"âœ… Dependencies installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: ğŸ” Setup Authentication (Optional)\n",
    "\n",
    "Set up authentication for experiment tracking and model sharing. These steps are optional but recommended.\n",
    "\n",
    "### Weights & Biases Setup:\n",
    "1. Go to [wandb.ai](https://wandb.ai) and create a free account\n",
    "2. Get your API key from [wandb.ai/authorize](https://wandb.ai/authorize)\n",
    "3. In Colab, go to the key icon (ğŸ”‘) on the left sidebar â†’ \"Secrets\"\n",
    "4. Add a new secret with name `WANDB_API_KEY` and paste your API key as the value\n",
    "5. Enable \"Notebook access\" for this secret\n",
    "\n",
    "### Hugging Face Hub Setup:\n",
    "1. Go to [huggingface.co](https://huggingface.co) and create a free account\n",
    "2. Get your access token from [huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)\n",
    "3. Create a new token with \"Write\" permissions\n",
    "4. In Colab Secrets, add a new secret with name `HUGGINGFACE_TOKEN` and paste your token\n",
    "5. Enable \"Notebook access\" for this secret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Weights & Biases (optional)\n",
    "try:\n",
    "    from google.colab import userdata\n",
    "    wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "    \n",
    "    if wandb_api_key:\n",
    "        import wandb\n",
    "        wandb.login(key=wandb_api_key)\n",
    "        print(\"âœ… Successfully logged into Weights & Biases!\")\n",
    "        use_wandb = True\n",
    "    else:\n",
    "        print(\"âš ï¸ WANDB_API_KEY not found in secrets. Skipping wandb setup.\")\n",
    "        print(\"ğŸ’¡ You can add it later in Colab Secrets for experiment tracking.\")\n",
    "        use_wandb = False\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not setup wandb: {e}\")\n",
    "    print(\"ğŸ’¡ Training will continue without wandb tracking.\")\n",
    "    use_wandb = False\n",
    "\n",
    "# Setup Hugging Face Hub (optional)\n",
    "try:\n",
    "    from huggingface_hub import login\n",
    "    hf_token = userdata.get('HF_TOKEN')\n",
    "    \n",
    "    if hf_token:\n",
    "        login(token=hf_token)\n",
    "        print(\"âœ… Successfully logged into Hugging Face Hub!\")\n",
    "        use_hf_hub = True\n",
    "    else:\n",
    "        print(\"âš ï¸ HUGGINGFACE_TOKEN not found in secrets. Skipping HF Hub setup.\")\n",
    "        print(\"ğŸ’¡ You can add it later in Colab Secrets for model sharing.\")\n",
    "        use_hf_hub = False\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not setup Hugging Face Hub: {e}\")\n",
    "    print(\"ğŸ’¡ Model will not be automatically pushed to Hugging Face.\")\n",
    "    use_hf_hub = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: ğŸ“¥ Dataset Preparation\n",
    "\n",
    "# Create directory for datasets\n",
    "!mkdir -p demo_data\n",
    "print(\"ğŸ“ Created demo_data directory\")\n",
    "\n",
    "# Download a sample dataset from Hugging Face\n",
    "# You can replace this with your own dataset\n",
    "print(\"â¬‡ï¸ Downloading dataset from Hugging Face...\")\n",
    "!huggingface-cli download \\\n",
    "    --repo-type dataset HelloCephalopod/block_pickup_17 \\\n",
    "    --local-dir ./demo_data/block_pickup_17\n",
    "print(\"âœ… Dataset downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the single camera modality configuration\n",
    "print(\"âš™ï¸ Setting up single camera modality configuration...\")\n",
    "!cp getting_started/examples/so100__modality.json ./demo_data/so101-table-cleanup/meta/modality.json\n",
    "print(\"âœ… Modality configuration set up!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the dataset structure\n",
    "print(\"ğŸ” Dataset structure:\")\n",
    "!ls -la ./demo_data/block_pickup_17\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the modality configuration\n",
    "import json\n",
    "with open('./demo_data/block_pickup_17/meta/modality.json', 'r') as f:\n",
    "    modality_config = json.load(f)\n",
    "print(\"âš™ï¸ Modality Configuration:\")\n",
    "print(json.dumps(modality_config, indent=2))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 4: ğŸ‘ï¸ Dataset Loading and Visualization\n",
    "\n",
    "Let's load the dataset and visualize some samples to understand the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the gr00t module to Python path\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from gr00t.data.dataset import LeRobotSingleDataset\n",
    "from gr00t.data.embodiment_tags import EmbodimentTag\n",
    "from gr00t.experiment.data_config import DATA_CONFIG_MAP\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(\"âœ… Modules imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "print(\"ğŸ“Š Loading dataset...\")\n",
    "dataset_path = \"./demo_data/block_pickup_17\"\n",
    "embodiment_tag = EmbodimentTag(\"new_embodiment\")\n",
    "\n",
    "# Get data configuration for single camera setup\n",
    "data_config_cls = DATA_CONFIG_MAP[\"so100\"]\n",
    "modality_configs = data_config_cls.modality_config()\n",
    "transforms = data_config_cls.transform()\n",
    "\n",
    "# Create dataset\n",
    "dataset = LeRobotSingleDataset(\n",
    "    dataset_path=dataset_path,\n",
    "    modality_configs=modality_configs,\n",
    "    transforms=transforms,\n",
    "    embodiment_tag=embodiment_tag,\n",
    "    video_backend=\"torchvision_av\",\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset loaded successfully!\")\n",
    "print(f\"ğŸ“ˆ Number of episodes: {len(dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample from the dataset\n",
    "print(\"ğŸ¬ Visualizing dataset sample...\")\n",
    "sample = dataset[0]\n",
    "print(\"ğŸ”‘ Sample keys:\", list(sample.keys()))\n",
    "\n",
    "# Display video frames\n",
    "if 'video.webcam' in sample:\n",
    "    video_frames = sample['video.webcam']\n",
    "    print(f\"ğŸ“¹ Video shape: {video_frames.shape}\")\n",
    "    \n",
    "    # Show first few frames\n",
    "    fig, axes = plt.subplots(1, min(5, video_frames.shape[0]), figsize=(15, 3))\n",
    "    if video_frames.shape[0] == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, ax in enumerate(axes[:min(5, video_frames.shape[0])]):\n",
    "        frame = video_frames[i].numpy()\n",
    "        # Convert from CxHxW to HxWxC\n",
    "        frame = np.transpose(frame, (1, 2, 0))\n",
    "        # Normalize to 0-1 range\n",
    "        frame = (frame - frame.min()) / (frame.max() - frame.min())\n",
    "        ax.imshow(frame)\n",
    "        ax.set_title(f'Frame {i}')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display state and action information\n",
    "if 'state.single_arm' in sample:\n",
    "    print(f\"ğŸ¦¾ State shape: {sample['state.single_arm'].shape}\")\n",
    "    print(f\"ğŸ“Š State values: {sample['state.single_arm'][0]}\")\n",
    "\n",
    "if 'action.single_arm' in sample:\n",
    "    print(f\"ğŸ¯ Action shape: {sample['action.single_arm'].shape}\")\n",
    "    print(f\"ğŸ“Š Action values: {sample['action.single_arm'][0]}\")\n",
    "\n",
    "if 'annotation.human.task_description' in sample:\n",
    "    print(f\"ğŸ“ Task description: {sample['annotation.human.task_description']}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 5: âš™ï¸ Fine-tuning Configuration\n",
    "\n",
    "Now let's set up the fine-tuning configuration. We'll use a configuration suitable for single camera setup and Colab's resource constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory for checkpoints\n",
    "!mkdir -p ./checkpoints\n",
    "print(\"ğŸ“ Created checkpoints directory\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning configuration\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "# Set environment variables for better performance\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "# Configure wandb reporting based on setup\n",
    "report_to = 'wandb' if use_wandb else 'none'\n",
    "\n",
    "# Fine-tuning command optimized for Colab\n",
    "cmd = [\n",
    "    'python', 'scripts/gr00t_finetune.py',\n",
    "    '--dataset-path', './demo_data/block_pickup_17',\n",
    "    '--num-gpus', '1',\n",
    "    '--output-dir', './checkpoints',\n",
    "    '--max-steps', '1000',  # Reduced for demo purposes\n",
    "    '--data-config', 'so100',  # Single camera configuration\n",
    "    '--video-backend', 'torchvision_av',\n",
    "    '--batch-size', '8',  # Reduced batch size for Colab\n",
    "    '--save-steps', '200',\n",
    "    '--learning-rate', '1e-4',\n",
    "    '--no-tune_diffusion_model',  # Disable diffusion model tuning to save memory\n",
    "    '--report-to', report_to  # Use wandb if available, otherwise none\n",
    "]\n",
    "\n",
    "print(\"ğŸ¯ Fine-tuning command configured:\")\n",
    "print(' '.join(cmd))\n",
    "print(\"\\nâš™ï¸ Configuration optimized for:\")\n",
    "print(\"   â€¢ Single camera setup (so100 config)\")\n",
    "print(\"   â€¢ Colab GPU memory constraints\")\n",
    "print(\"   â€¢ Reduced training steps for demo\")\n",
    "print(f\"   â€¢ Experiment tracking: {'Weights & Biases' if use_wandb else 'Disabled'}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 6: ğŸš€ Start Fine-tuning\n",
    "\n",
    "Now let's start the fine-tuning process. This will take some time depending on your hardware.\n",
    "\n",
    "If you set up Weights & Biases, you'll be able to monitor training progress in real-time at [wandb.ai](https://wandb.ai).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "print(\"ğŸš€ Starting fine-tuning...\")\n",
    "print(\"â³ This may take a while. You can monitor progress in the output below.\")\n",
    "print(\"â˜• Time for a coffee break!\\n\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "    print(\"ğŸ‰ Fine-tuning completed successfully!\")\n",
    "    print(\"ğŸ“Š Output:\", result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"âŒ Fine-tuning failed with error: {e}\")\n",
    "    print(f\"ğŸ” Error output: {e.stderr}\")\n",
    "    print(\"\\nğŸ”§ Trying with reduced memory settings...\")\n",
    "    \n",
    "    # Try with even more reduced settings\n",
    "    cmd_reduced = cmd + ['--batch-size', '4', '--tune_projector', 'False']\n",
    "    try:\n",
    "        result = subprocess.run(cmd_reduced, capture_output=True, text=True, check=True)\n",
    "        print(\"ğŸ‰ Fine-tuning completed successfully with reduced settings!\")\n",
    "        print(\"ğŸ“Š Output:\", result.stdout)\n",
    "    except subprocess.CalledProcessError as e2:\n",
    "        print(f\"âŒ Fine-tuning still failed: {e2}\")\n",
    "        print(f\"ğŸ” Error output: {e2.stderr}\")\n",
    "        print(\"\\nğŸ’¡ Try reducing batch size further or using a different GPU runtime.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 7: ğŸ“Š Check Training Progress\n",
    "\n",
    "Let's check if the training completed and examine the checkpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if checkpoints were created\n",
    "print(\"ğŸ“ Checking for checkpoints...\")\n",
    "!ls -la ./checkpoints/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest checkpoint\n",
    "import glob\n",
    "checkpoint_dirs = glob.glob('./checkpoints/checkpoint-*')\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "    print(f\"âœ… Latest checkpoint found: {latest_checkpoint}\")\n",
    "    !ls -la {latest_checkpoint}\n",
    "else:\n",
    "    print(\"âŒ No checkpoints found. Training may have failed or not completed yet.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 8: ğŸ“ˆ Model Evaluation\n",
    "\n",
    "If training completed successfully, let's evaluate the fine-tuned model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the fine-tuned model (if checkpoints exist)\n",
    "if checkpoint_dirs:\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "    \n",
    "    eval_cmd = [\n",
    "        'python', 'scripts/eval_policy.py',\n",
    "        '--plot',\n",
    "        '--embodiment_tag', 'new_embodiment',\n",
    "        '--model_path', latest_checkpoint,\n",
    "        '--data_config', 'so100',\n",
    "        '--dataset_path', './demo_data/block_pickup_17',\n",
    "        '--video_backend', 'torchvision_av',\n",
    "        '--modality_keys', 'single_arm', 'gripper'\n",
    "    ]\n",
    "    \n",
    "    print(\"ğŸ“Š Evaluating fine-tuned model...\")\n",
    "    print(' '.join(eval_cmd))\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(eval_cmd, capture_output=True, text=True, check=True)\n",
    "        print(\"ğŸ‰ Evaluation completed successfully!\")\n",
    "        print(\"ğŸ“Š Output:\", result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"âŒ Evaluation failed: {e}\")\n",
    "        print(f\"ğŸ” Error output: {e.stderr}\")\n",
    "else:\n",
    "    print(\"âŒ No checkpoints available for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 9: ğŸ’¾ Save and Download Model\n",
    "\n",
    "If you want to save your fine-tuned model, you can download it from Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a zip file of the checkpoints for download\n",
    "if checkpoint_dirs:\n",
    "    print(\"ğŸ“¦ Creating zip file for download...\")\n",
    "    !zip -r gr00t_finetuned_model.zip ./checkpoints/\n",
    "    print(\"âœ… Model checkpoints zipped as 'gr00t_finetuned_model.zip'\")\n",
    "    print(\"ğŸ“¥ You can download this file from the Colab file browser.\")\n",
    "    \n",
    "    # Show file size\n",
    "    !ls -lh gr00t_finetuned_model.zip\n",
    "else:\n",
    "    print(\"âŒ No checkpoints to save.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 10: ğŸš€ Push Model to Hugging Face Hub\n",
    "\n",
    "If you set up Hugging Face authentication, we can automatically push your fine-tuned model to the Hub for sharing and future use.\n",
    "\n",
    "## ğŸ“‹ Summary\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. âœ… Set up the environment with Isaac-GR00T\n",
    "2. âœ… Configured authentication for Weights & Biases and Hugging Face\n",
    "3. âœ… Downloaded a dataset from Hugging Face\n",
    "4. âœ… Configured the dataset for single camera setup\n",
    "5. âœ… Loaded and visualized the dataset\n",
    "6. âœ… Fine-tuned GR00T N1.5 on the dataset with monitoring\n",
    "7. âœ… Evaluated the fine-tuned model\n",
    "8. âœ… Saved the model locally\n",
    "9. âœ… Automatically pushed the model to Hugging Face Hub\n",
    "\n",
    "## ğŸš€ Next Steps\n",
    "\n",
    "To extend this work, you can:\n",
    "\n",
    "- **ğŸ”„ Use your own dataset**: Replace the Hugging Face dataset with your own robot data\n",
    "- **âš™ï¸ Experiment with hyperparameters**: Try different learning rates, batch sizes, and training steps\n",
    "- **ğŸ“Š Advanced monitoring**: Set up custom wandb metrics and alerts\n",
    "- **ğŸ¤– Deploy to real hardware**: Use the fine-tuned model with actual robot hardware\n",
    "- **ğŸ“¹ Multi-camera setup**: Modify the modality configuration for multiple cameras\n",
    "- **ğŸ¦¾ Different robot embodiments**: Adapt the configuration for different robot types\n",
    "- **ğŸ”¬ Hyperparameter sweeps**: Use wandb sweeps to automatically find optimal hyperparameters\n",
    "- **ğŸŒ Share your models**: Upload your fine-tuned models to Hugging Face for the community\n",
    "- **ğŸ”„ Model versioning**: Use Hugging Face's model versioning for iterative improvements\n",
    "\n",
    "## ğŸ“š Resources\n",
    "\n",
    "- [Isaac-GR00T GitHub Repository](https://github.com/NVIDIA/Isaac-GR00T)\n",
    "- [GR00T N1.5 Technical Blog](https://research.nvidia.com/labs/gear/gr00t-n1_5/)\n",
    "- [LeRobot Framework](https://github.com/huggingface/lerobot)\n",
    "- [Hugging Face Datasets](https://huggingface.co/datasets)\n",
    "- [NVIDIA GR00T Models on Hugging Face](https://huggingface.co/nvidia)\n",
    "- [Weights & Biases Documentation](https://docs.wandb.ai/)\n",
    "- [Weights & Biases for ML Experiments](https://wandb.ai/site/experiment-tracking)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ‰ Congratulations!\n",
    "\n",
    "You've successfully fine-tuned GR00T N1.5 with a single camera setup and experiment tracking! ğŸ¤–âœ¨\n",
    "\n",
    "**Happy fine-tuning and robot building!** ğŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push fine-tuned model to Hugging Face Hub\n",
    "if use_hf_hub and checkpoint_dirs:\n",
    "    latest_checkpoint = max(checkpoint_dirs, key=os.path.getctime)\n",
    "    \n",
    "    # Get user input for model repository name\n",
    "    import getpass\n",
    "    print(\"ğŸ·ï¸ Model Repository Setup\")\n",
    "    print(\"Enter details for your Hugging Face model repository:\")\n",
    "    \n",
    "    # You can customize these or make them interactive\n",
    "    default_username = \"your-username\"  # Users should replace this\n",
    "    default_model_name = \"gr00t-n1.5-so100-finetuned\"\n",
    "    \n",
    "    print(f\"ğŸ“ Suggested repository name: {default_username}/{default_model_name}\")\n",
    "    print(\"ğŸ’¡ You can change this in the code above if needed\")\n",
    "    \n",
    "    repo_name = f\"{default_username}/{default_model_name}\"\n",
    "    \n",
    "    try:\n",
    "        from huggingface_hub import HfApi, create_repo\n",
    "        \n",
    "        # Create repository if it doesn't exist\n",
    "        print(f\"ğŸ“ Creating repository: {repo_name}\")\n",
    "        create_repo(repo_id=repo_name, exist_ok=True, private=False)\n",
    "        \n",
    "        # Upload the model files\n",
    "        api = HfApi()\n",
    "        print(f\"â¬†ï¸ Uploading model files from {latest_checkpoint}...\")\n",
    "        \n",
    "        # Upload all files in the checkpoint directory\n",
    "        api.upload_folder(\n",
    "            folder_path=latest_checkpoint,\n",
    "            repo_id=repo_name,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=f\"Fine-tuned GR00T N1.5 model on SO-101 dataset\"\n",
    "        )\n",
    "        \n",
    "        # Create a model card\n",
    "        model_card_content = f\"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: nvidia/GR00T-N1.5-3B\n",
    "tags:\n",
    "- robotics\n",
    "- gr00t\n",
    "- fine-tuned\n",
    "- so-101\n",
    "- single-camera\n",
    "library_name: transformers\n",
    "---\n",
    "\n",
    "# GR00T N1.5 Fine-tuned on SO-101 Dataset\n",
    "\n",
    "This model is a fine-tuned version of [nvidia/GR00T-N1.5-3B](https://huggingface.co/nvidia/GR00T-N1.5-3B) on a SO-101 robot arm dataset with single camera setup.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: nvidia/GR00T-N1.5-3B\n",
    "- **Fine-tuned on**: SO-101 table cleanup dataset\n",
    "- **Camera Setup**: Single camera (webcam)\n",
    "- **Embodiment**: SO-101 robot arm\n",
    "- **Training Steps**: 1000 (demo configuration)\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from gr00t.model.gr00t_n1 import GR00T_N1_5\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = GR00T_N1_5.from_pretrained(\"{repo_name}\")\n",
    "\n",
    "# Use for inference on SO-101 robot\n",
    "# ... your inference code here\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Training Data**: SO-100 pick and place block task\n",
    "- **Training Steps**: 1000\n",
    "- **Batch Size**: 8\n",
    "- **Learning Rate**: 1e-4\n",
    "- **Hardware**: Google Colab GPU\n",
    "\n",
    "## Intended Use\n",
    "\n",
    "This model is intended for research and educational purposes with SO-101 robot arms performing table cleanup tasks.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Trained on a small dataset (demo purposes)\n",
    "- Single camera setup only\n",
    "- Specific to SO-100 embodiment\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model, please cite the original GR00T paper and acknowledge the fine-tuning work.\n",
    "\"\"\"\n",
    "        \n",
    "        # Upload model card\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=model_card_content.encode(),\n",
    "            path_in_repo=\"README.md\",\n",
    "            repo_id=repo_name,\n",
    "            repo_type=\"model\",\n",
    "            commit_message=\"Add model card\"\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ‰ Successfully uploaded model to: https://huggingface.co/{repo_name}\")\n",
    "        print(f\"ğŸ”— Model URL: https://huggingface.co/{repo_name}\")\n",
    "        print(\"âœ… Model card created with training details\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to upload to Hugging Face: {e}\")\n",
    "        print(\"ğŸ’¡ You can manually upload the model files later\")\n",
    "        \n",
    "elif not use_hf_hub:\n",
    "    print(\"âš ï¸ Hugging Face Hub not configured. Skipping model upload.\")\n",
    "    print(\"ğŸ’¡ Set up HUGGINGFACE_TOKEN in Colab Secrets to enable automatic uploads\")\n",
    "elif not checkpoint_dirs:\n",
    "    print(\"âŒ No model checkpoints found to upload.\")\n",
    "else:\n",
    "    print(\"â„¹ï¸ Model upload skipped.\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## ğŸ“Š Monitoring Training with Weights & Biases\n",
    "\n",
    "If you set up Weights & Biases, you can monitor your training progress in real-time:\n",
    "\n",
    "1. **Visit your wandb dashboard**: Go to [wandb.ai](https://wandb.ai) and navigate to your project\n",
    "2. **Monitor key metrics**: \n",
    "   - Training loss\n",
    "   - Learning rate schedule\n",
    "   - GPU utilization\n",
    "   - Training speed (steps/sec)\n",
    "3. **Compare experiments**: Try different hyperparameters and compare results\n",
    "4. **Share results**: Generate reports and share with your team\n",
    "\n",
    "### Key Metrics to Watch:\n",
    "- **Loss should decrease**: Training loss should trend downward over time\n",
    "- **No overfitting**: Validation loss (if available) shouldn't increase while training loss decreases\n",
    "- **Stable training**: Loss shouldn't oscillate wildly or explode\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
